{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NOTEBOOK 1**"
      ],
      "metadata": {
        "id": "QBvEy7uew4aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este notebook encontraras algunos de los metodos para imputar valores dentro de un DataFrame, así como los primeros paso para un EDA, adicional de ver algunas maneras de como normalizar o estandarizar los valores dentro de un dataset, com ultimo se muestran algunas formas para tratar la multidimensionalidad y reducir esta.\n",
        "\n",
        "Recuerde que esto solo es un Notebook que no contiene el fundamento teorico, muchas de la información fue adquirida del libro: \"Estadistica para Ciencia de Datos con R y Python\""
      ],
      "metadata": {
        "id": "SShM-Iq1vFsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTACION DE LIBRERIAS\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "zYnBqhH-YEkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANALISIS DE DATOS**"
      ],
      "metadata": {
        "id": "g2AIt-L-0_0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame() #Hacer un dataframe\n",
        "\n",
        "df.var() #Muestra las varianzas de los valores numericos del df (hay que seleccionar los valores numericos)\n",
        "\n",
        "df = df.drop('Column1', axis=1) # Borra una columna\n",
        "\n",
        "df.describe(exclude[object]) #Describe un dataframe sin los objetos\n",
        "\n",
        "df2 = df.copy() #Realiza una copia del dataframe\n",
        "\n",
        "df['Column1'].value_counts() #Lista las variables con el numero de valores contenidos en el df\n",
        "\n",
        "df['Column1'].isnull() #Muestra si tiene valores nulos en dicha columna\n",
        "\n",
        "df['Column1'].duplicated() #Observar valores duplicados\n",
        "\n",
        "#rellenar campos de un data frame con valores aleatorios\n",
        "df['Pais'] = np.random.choise(['Alemania', 'Brasil', 'Japon'], len(df), p =[0.3, 0.3, 0.4])\n",
        "\n",
        "#JUNTAR DOS DATA FRAMES\n",
        "df = pd.concat([df1, df2], axis=1)\n"
      ],
      "metadata": {
        "id": "-ULi8kPUV7kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CAMBIAR LOS TIPOS DE DATOS DE UN DF\n",
        "df = df.astype({\n",
        "    'Column1':'float64',\n",
        "    'Column2':'int64'\n",
        "    'Column3':'String'\n",
        "})"
      ],
      "metadata": {
        "id": "l_NZIFnEGWii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J99b4yIi-EwT"
      },
      "outputs": [],
      "source": [
        "# Toma todas las columnas de valores categoricos\n",
        "categorical = [column_name for column_name in df.columns if df[column_name].dtyoe = \"O\"]\n",
        "# Suma los valores nulos de cada columna\n",
        "df[categorical].isnull().sum()\n",
        "\n",
        "# Toma todas las columnas de valores que no son categoricos\n",
        "categorical = [column_name for column_name in df.columns if df[column_name].dtyoe != \"O\"]\n",
        "# Suma los valores nulos de cada columna no categorica\n",
        "df[categorical].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPUTACION DE VALORES**\n",
        "**Rellenar valores nulos, faltantes, o eliminar registros**"
      ],
      "metadata": {
        "id": "fi3nL6eL5Prg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rellena los valores nulos con los valores de su media\n",
        "df = df.fillna(df.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "XKEar69J5bmL",
        "outputId": "4f25751e-8ae9-44de-ec68-abd28ff3dbfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2783dd0c2146>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDAR VALORES DUPLICADOS\n",
        "def validar_duplicados(df):\n",
        "    # Comprueba si hay duplicados en el DataFrame\n",
        "    duplicated = df.duplicated()\n",
        "    if duplicated.any():\n",
        "        print(\"Se encontraron duplicados:\")\n",
        "        print(df[duplicated])\n",
        "    else:\n",
        "        print(\"No se encontraron duplicados.\")\n",
        "validar_duplicados(df)"
      ],
      "metadata": {
        "id": "Qzw_9e18nBeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificamos la completitud de los valores\n",
        "completitud = pd.DataFrame(df.isnull().sum())  # Contamos los valores nulos por columna\n",
        "completitud.reset_index(inplace=True)  # Restablecemos el índice para convertirlo en columna\n",
        "completitud = completitud.rename(columns={\"index\": \"columna\", 0: \"total\"})  # Renombramos las columnas\n",
        "completitud[\"completitud\"] = (1 - completitud[\"total\"] / df.shape[0]) * 100  # Calculamos el porcentaje de completitud\n",
        "completitud = completitud.sort_values(by=\"completitud\", ascending=True)  # Ordenamos por completitud (de menor a mayor)\n",
        "completitud.reset_index(drop=True, inplace=True)  # Restablecemos el índice después de ordenar"
      ],
      "metadata": {
        "id": "wwkwgvs8nSef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BORRAR VARIABLES CON MENOS DEL 80% DE COMPLETITUD\n",
        "null_cols = list(completitud[completitud[\"completitud\"] < 80][\"columna\"].values)  # Filtramos las columnas con < 80% de completitud\n",
        "df = df.drop(columns=null_cols)  # Eliminamos las columnas seleccionadas\n",
        "\n",
        "# Aseguramos que no tenemos variables con <80% de completitud\n",
        "completitud = pd.DataFrame(df.isnull().sum())  # Calculamos los valores nulos por columna\n",
        "completitud.reset_index(inplace=True)  # Restablecemos el índice para convertirlo en columna\n",
        "completitud = completitud.rename(columns={\"index\": \"columna\", 0: \"total\"})  # Renombramos las columnas\n",
        "completitud[\"completitud\"] = (1 - completitud[\"total\"] / df.shape[0]) * 100  # Calculamos el porcentaje de completitud\n",
        "completitud = completitud.sort_values(by=\"completitud\", ascending=True)  # Ordenamos por completitud (de menor a mayor)\n",
        "completitud.reset_index(drop=True, inplace=True)  # Restablecemos el índice después de ordenar"
      ],
      "metadata": {
        "id": "vh-ocvMqnXuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para tomar los outliers y meterlos en el box\n",
        "outliers = ['Column1', 'Column2', 'Column3']  # Lista de las columnas a evaluar por outliers\n",
        "for feature in outliers:\n",
        "    q1 = df[feature].quantile(0.25)  # Cuartil 1 (25%)\n",
        "    q3 = df[feature].quantile(0.75)  # Cuartil 3 (75%)\n",
        "    IQR = q3 - q1  # Rango intercuartil (IQR)\n",
        "    lower_limit = q1 - (IQR * 1.5)  # Límite inferior para detectar outliers\n",
        "    upper_limit = q3 + (IQR * 1.5)  # Límite superior para detectar outliers\n",
        "\n",
        "    # Reemplazar los valores por los límites\n",
        "    df.loc[df[feature] < lower_limit, feature] = lower_limit  # Valores menores que el límite inferior\n",
        "    df.loc[df[feature] > upper_limit, feature] = upper_limit  # Valores mayores que el límite superior"
      ],
      "metadata": {
        "id": "hV9_xplRAHQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rellenar los valores nulos con valores promedios\n",
        "numerical_features_with_null = [feature for feature in numerical_features if df[feature].isnull().sum()]  # Identificar columnas numéricas con valores nulos\n",
        "\n",
        "for feature in numerical_features_with_null:\n",
        "    mean_value = df[feature].mean()  # Calcular el valor promedio de la columna\n",
        "    df[feature].fillna(mean_value, inplace=True)  # Rellenar los valores nulos con el valor promedio"
      ],
      "metadata": {
        "id": "nd0nKqePJIQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputación de valores faltantes\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "# Identificar las columnas con valores faltantes (no 100% completas)\n",
        "imputar = list(completitud[completitud[\"completitud\"] != 100][\"columna\"].values)\n",
        "\n",
        "# Crear una instancia de SimpleImputer para imputar los valores faltantes con el valor más frecuente\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
        "\n",
        "# Ajustamos el imputador a las columnas con valores faltantes\n",
        "imp.fit(df[imputar])\n",
        "\n",
        "# Aplicamos la imputación de los valores faltantes\n",
        "df[imputar] = imp.transform(df[imputar])"
      ],
      "metadata": {
        "id": "VNeDYmFqnj2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[imputar]=imp.transform(df[imputar]) #Aderimos los valores faltantes"
      ],
      "metadata": {
        "id": "r1NkyTBgnozB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificamos la completitud de los valores en cada una de las columnas del DataFrame\n",
        "completitud = pd.DataFrame(df.isnull().sum())  # Contamos los valores nulos por columna\n",
        "completitud.reset_index(inplace=True)  # Restablecemos el índice para convertirlo en columna\n",
        "completitud = completitud.rename(columns={\"index\": \"columna\", 0: \"total\"})  # Renombramos las columnas: 'columna' y 'total'\n",
        "completitud[\"completitud\"] = (1 - completitud[\"total\"] / df.shape[0]) * 100  # Calculamos el porcentaje de completitud (porcentaje de no nulos)\n",
        "completitud = completitud.sort_values(by=\"completitud\", ascending=True)  # Ordenamos el DataFrame por la columna 'completitud' (de menor a mayor)\n",
        "completitud.reset_index(drop=True, inplace=True)  # Restablecemos el índice después de ordenar, para numerar las filas de forma secuencial\n",
        "completitud  # Mostramos el DataFrame con la completitud de cada columna"
      ],
      "metadata": {
        "id": "NwXf_-q_nsNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ESCALADORES**"
      ],
      "metadata": {
        "id": "MVBqK7YJ06D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NORMALIZAR\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "norm = Normalizer()\n",
        "df_norm = norm.fit_transform(df) # esto es un array\n",
        "df_norm = pd.DataFrame(df_norm, columns=df.columns) #al array lo vuelvo DF"
      ],
      "metadata": {
        "id": "TOjSJklz4ao7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ESCALADORES\n",
        "\n",
        "# Standard Scaler\n",
        "escalador = StandardScaler()\n",
        "transformado = escalador.fit_transform(df) # Tomar valores numericos\n",
        "transformado\n",
        "\n",
        "#MINMAX Escaler\n",
        "escalador = MinMaxScaler()\n",
        "transformado= escalador.fit_transform(df)\n",
        "transformado"
      ],
      "metadata": {
        "id": "WDMJJgvj-s1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRANSFORMACION DE VALORES CATEGORICOS**"
      ],
      "metadata": {
        "id": "snvjjy3C0vvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMADORES DE VARIABLES CATEGORICAS\n",
        "# Pone una etiqueta númerica a una variable categorica\n",
        "\n",
        "# LABEL ENCONDER\n",
        "from sklearn.processing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "#Se instancian al crear un objeto con la clase LabelEncoder\n",
        "Le_En = LabelEncoder()\n",
        "\n",
        "#Lo usual es que se muestre los valores unicos de una variable\n",
        "df['Column1'].nunique() #numero de valores unicos\n",
        "df['Column1'].unique() #lista de valores unicos\n",
        "\n",
        "df['Column1'] = Le_En.fit_transform(df['Column1'])\n",
        "#Muestra las clases del label encoder\n",
        "print(Le_En.classes_)\n",
        "\n",
        "#Muestra como lista las clases\n",
        "for i in range(len(Le_En.classes_)):\n",
        "  print(f'{i} - {Le_En.classes_{i}}')\n",
        "\n",
        "df['Column1'] = Le_En.inverse_transform(df['Column1']) #Transformación inversa\n",
        "\n",
        "#ONEHOTENCODER\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "#Se instancia el transformador\n",
        "transformador = OneHotEncoder()\n",
        "\n",
        "enc_df = pd.DataFrame(transformador.fit_transform(df[['sex']]).toarray())\n",
        "\n",
        "df = df.join(enc_df) #unimos los dos dataframes enc_df y df\n",
        "\n",
        "# Dummies\n",
        "dummies = pd.get_dummies(df, columns=[\"sex\"], prefix=[\"sex\"] )\n",
        "dummies[['sex_male','sex_female']]\n",
        "df"
      ],
      "metadata": {
        "id": "Fcz1BV7NGyqj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "d2ea744c-480d-4cb2-ffd2-6c19384014fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "f-string: invalid syntax. Perhaps you forgot a comma? (<ipython-input-2-02a7e56ccfe8>, line 20)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-02a7e56ccfe8>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    print(f'{i} - {Le_En.classes_{i}}')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**REDUCCION DE DIMENSIONES**\n"
      ],
      "metadata": {
        "id": "F2XFntve0fYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CORRELACION**"
      ],
      "metadata": {
        "id": "tDVIdP0VhfYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#REALIZA LA MATRIZ DE COVARIANZA\n",
        "df_cov = df.cov()\n",
        "#Graficamos la matriz de covarianza\n",
        "sns.heatmap(df.cov)\n"
      ],
      "metadata": {
        "id": "5zsThk0OhkBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VIF FACTOR DE INFLACION DE LA VARIANZA**"
      ],
      "metadata": {
        "id": "1xg28uPciQES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FACTOR DE INFLACION DE LA VARIANZA VIF\n",
        "import statsmodels.api as sm  # Importamos statsmodels para trabajar con modelos estadísticos\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor  # Importamos la función para calcular el VIF\n",
        "\n",
        "# Calculamos el VIF\n",
        "vif = sm.add_constant(df)  # Añadimos una constante (columna de unos) al DataFrame para incluir el intercepto en el modelo\n",
        "vif = [variance_inflation_factor(vif.values, i) for i in range(vif.shape[1])]  # Calculamos el VIF para cada columna (inclusive la constante)\n",
        "vif = pd.Series(vif, index = sm.add_constant(df).columns)  # Convertimos los resultados en una serie, con los nombres de las columnas como índice\n",
        "vif  # Mostramos el VIF para cada variable"
      ],
      "metadata": {
        "id": "Hpj8ZfpliMpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA**"
      ],
      "metadata": {
        "id": "nD7Zw-JaiANi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA\n",
        "from sklearn.decomposition import PCA\n",
        "#Se puede añadir ruido\n",
        "pca = PCA() #opcional se puede agregar un argumento numerico para elegir el numero de variables que se deseas.\n",
        "pca.fit_transform(df)\n",
        "\n",
        "#Acumulacion de varianzas y grafico para elegir las variables\n",
        "\n",
        "ar_var = pca.explained_variance_ratio_.cumsum()\n",
        "ar_pca_sum = np.cumsum(ar_var)\n",
        "\n",
        "#Grafico\n",
        "plt.plot(ar_pca_sum)\n",
        "plt.xlabel('N componentes')\n",
        "plt.ylabel('Suma acumulada de varianzas')\n",
        "plt.show()\n",
        "\n",
        "#Elegir la cantidad de variables\n",
        "np.where((ar_pca_sum > 0.8) & (ar_pca_sum < 0.9))\n",
        "pca_variance = pca.explained_variance_ #explicara cada una de las varianzas de cada variable\n",
        "pca_variance"
      ],
      "metadata": {
        "id": "ur-DczuV6Gmj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}