{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SFbKsU9pZALe",
        "fDwkWMR50VHA",
        "6f7HKNU_0Yp-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **REDES NEURONALES**"
      ],
      "metadata": {
        "id": "SFbKsU9pZALe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejemplo Celulares**\n"
      ],
      "metadata": {
        "id": "KdAaqkIRvez-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X532YAiLuL9_"
      },
      "outputs": [],
      "source": [
        "#Instalamos SCIPY y TensorFlow porque no vienen de manera predeterminada en Python\n",
        "!pip install scipy\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importamos las librerías instaladas\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "bDtF7k1duhaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "aYfPPkUzuobV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "tSYwX6e5uqTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xeqS3_ySuvgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "Qg5YJR-Ouwpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/ucamp-ds-datasets/mobile_price/train.csv\") #Cambiar ruta a la disponible localmente"
      ],
      "metadata": {
        "id": "HZcGW9sruylB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "SYQ7dp9Pu35r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=['price_range'])\n",
        "y = df[\"price_range\"]"
      ],
      "metadata": {
        "id": "O5DVVaEmu5f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "4qDR9wN2u60r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "_hNFsNf7u9A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "-QqzbeTlu-fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tensorflow - Keras**"
      ],
      "metadata": {
        "id": "fDwkWMR50VHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EJEMPLO 1**"
      ],
      "metadata": {
        "id": "Q0Ef2bB1Xrvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Arquitectura de Red**"
      ],
      "metadata": {
        "id": "ffgVqtenvAQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (X.shape[1],)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_shape=(20,), activation=\"relu\"))\n",
        "model.add(Dense(8, activation=\"relu\"))\n",
        "model.add(Dense(4, activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "pfPacGKZvEDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compilado de la red**"
      ],
      "metadata": {
        "id": "sOtYwpsSvGgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "aH-IRMhWvGJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento de la red**"
      ],
      "metadata": {
        "id": "LK2gaoIUvMeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "batch_size = 10\n",
        "\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=10)"
      ],
      "metadata": {
        "id": "vmJqn7ZFvOZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evualación**"
      ],
      "metadata": {
        "id": "3Yl9hfQHvRIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Precisión: %.2f' % (accuracy*100))"
      ],
      "metadata": {
        "id": "Z9wU4-2KvQyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicciones**"
      ],
      "metadata": {
        "id": "2UeFzsBnvWMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "id": "I07MRk0tvasL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for prediction, y_real in zip(predictions, y_test):\n",
        "    print(f\"Prediccion: {np.argmax(prediction)}, Real: {y_real}\")"
      ],
      "metadata": {
        "id": "JRKff7x7vc_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EJEMPLO 2**"
      ],
      "metadata": {
        "id": "5vujT4wDXv9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "4T8PDSI2Xyqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
        "df = read_csv(path, header=None)"
      ],
      "metadata": {
        "id": "nGAjPlLjYEzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividimos las columnas de entrada y salida\n",
        "X, y = df.values[:, :-1], df.values[:, -1]\n",
        "# Nos aseguramos de que todos los datos sean valores flotantes\n",
        "X = X.astype('float32')"
      ],
      "metadata": {
        "id": "sfSYyBldYFhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cambiamos los strings a integers\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "y"
      ],
      "metadata": {
        "id": "8ldAz1SFYKgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividimos los datos en entrenamiento y test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "YBz6Tg9WYNAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determinamos el número de caracteristicas de entrada.\n",
        "n_features = X_train.shape[1]"
      ],
      "metadata": {
        "id": "kZ9qSRKUYPrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el modelo\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "X15a5ajtYRgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilamos el modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "90mx2ePPYTgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el modelo\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "id": "btjK1MLEYURr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluamos el modelo\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Presición Test: %.3f' % acc)"
      ],
      "metadata": {
        "id": "7MbShShMYV5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos predicciones\n",
        "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
        "yhat = model.predict([row])\n",
        "print('Predecido: %.3f' % yhat)"
      ],
      "metadata": {
        "id": "1FiVfY5kYaHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Para clasificacion multiclase**"
      ],
      "metadata": {
        "id": "N0_e877pYi8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilamos el modelo\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "OGDsNHQ7YoXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PYTORCH**"
      ],
      "metadata": {
        "id": "6f7HKNU_0Yp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Ejemplo 1**"
      ],
      "metadata": {
        "id": "ycIa0nFrVBzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch es una biblioteca de machine learning que facilita trabajar con tensores y redes neuronales.\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# **1. Importar librerías necesarias**\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "import torch  # Biblioteca principal de PyTorch\n",
        "import torch.nn as nn  # Módulo para construir redes neuronales\n",
        "import torch.optim as optim  # Optimización para redes neuronales\n",
        "import torch.nn.functional as F  # Funciones predefinidas para redes neuronales\n",
        "from torch.utils.data import DataLoader, Dataset  # Utilidades para manejar datasets\n",
        "\n",
        "# **5. REDES NEURONALES**\n",
        "# -----------------------------------------------------------\n",
        "# Crear una red neuronal simple usando `torch.nn`\n",
        "\n",
        "# Definimos una red neuronal personalizada\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        # Capas lineales\n",
        "        self.fc1 = nn.Linear(2, 4)  # Capa con 2 entradas y 4 salidas\n",
        "        self.fc2 = nn.Linear(4, 1)  # Capa con 4 entradas y 1 salida\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))  # Activación ReLU después de la primera capa\n",
        "        x = self.fc2(x)  # Salida final\n",
        "        return x\n",
        "\n",
        "# Instanciamos la red\n",
        "red = SimpleNN()\n",
        "print(\"\\nEstructura de la red:\")\n",
        "print(red)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# **6. ENTRENAMIENTO DE UNA RED NEURONAL**\n",
        "# -----------------------------------------------------------\n",
        "# Ahora veremos cómo entrenar una red neuronal para resolver un problema de regresión.\n",
        "\n",
        "# Crear datos sintéticos\n",
        "x_train = torch.rand(100, 2)  # 100 muestras, 2 características\n",
        "y_train = torch.sum(x_train, dim=1, keepdim=True)  # Etiquetas: suma de las características\n",
        "\n",
        "# Definir la red, función de pérdida y optimizador\n",
        "modelo = SimpleNN()\n",
        "criterio = nn.MSELoss()  # Función de pérdida: Error cuadrático medio\n",
        "optimizador = optim.SGD(modelo.parameters(), lr=0.01)  # Optimizador SGD con tasa de aprendizaje 0.01\n",
        "\n",
        "# Entrenamiento\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # Paso hacia adelante\n",
        "    predicciones = modelo(x_train)  # Predicciones del modelo\n",
        "    perdida = criterio(predicciones, y_train)  # Cálculo de la pérdida\n",
        "\n",
        "    # Paso hacia atrás\n",
        "    optimizador.zero_grad()  # Limpiar gradientes previos\n",
        "    perdida.backward()  # Calcular gradientes\n",
        "    optimizador.step()  # Actualizar pesos\n",
        "\n",
        "    # Mostrar progreso\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Época [{epoch+1}/{num_epochs}], Pérdida: {perdida.item():.4f}\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# **7. EVALUACIÓN Y USO DEL MODELO ENTRENADO**\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# Datos de prueba\n",
        "x_test = torch.tensor([[0.2, 0.8], [0.5, 0.5]])\n",
        "y_test = modelo(x_test)\n",
        "print(\"\\nPredicciones del modelo para datos de prueba:\\n\", y_test)\n"
      ],
      "metadata": {
        "id": "7A71iM4NLfdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pythorch paso a paso**"
      ],
      "metadata": {
        "id": "4kANGg2xUV0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import vstack\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "Czs8xI0-UqAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importamos las bibliotecas de Pythorch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Sigmoid\n",
        "from torch.nn import Module\n",
        "from torch.optim import SGD\n",
        "from torch.nn import BCELoss\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_"
      ],
      "metadata": {
        "id": "9mdFB7OkUVZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparación de datos**"
      ],
      "metadata": {
        "id": "SRYh_z67U4_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de nuestro dataset\n",
        "class CSVDataset(Dataset):\n",
        "    # Cargamos el dataset\n",
        "    def __init__(self, path):\n",
        "        # Cargamos el archivo CSV como un dataframe\n",
        "        df = read_csv(path, header=None)\n",
        "        # Guardamos los valores de entrada y los de salida\n",
        "        self.X = df.values[:, :-1]\n",
        "        self.y = df.values[:, -1]\n",
        "        # Nos aseguramos de que la data de entrada sea flotante\n",
        "        self.X = self.X.astype('float32')\n",
        "        # Etiquetamos el objetivo de codificación y asegúrese de que los valores sean flotantes\n",
        "        self.y = LabelEncoder().fit_transform(self.y)\n",
        "        self.y = self.y.astype('float32')\n",
        "        self.y = self.y.reshape((len(self.y), 1))\n",
        "\n",
        "    # Sacamos el número de filas en el dataset\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    # Obtenemos una fila en un índice\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.X[idx], self.y[idx]]\n",
        "\n",
        "    # Obtenemos los índices para entrenar y probar filas\n",
        "    def get_splits(self, n_test=0.33):\n",
        "        # Determinamos los tamaños\n",
        "        test_size = round(n_test * len(self.X))\n",
        "        train_size = len(self.X) - test_size\n",
        "        # Calculamos los splits\n",
        "        return random_split(self, [train_size, test_size])\n",
        "\n",
        "# Preparamos el dataset\n",
        "def prepare_data(path):\n",
        "    # Cargamos el dataset\n",
        "    dataset = CSVDataset(path)\n",
        "    # Calculamos los splits\n",
        "    train, test = dataset.get_splits()\n",
        "    # Preparamos cargadores de datos\n",
        "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
        "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
        "    return train_dl, test_dl"
      ],
      "metadata": {
        "id": "mKZoUl_wUcob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definición del modelo**"
      ],
      "metadata": {
        "id": "CnX2MhplUx2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición del modelo\n",
        "class MLP(Module):\n",
        "    # Definimos los elementos del modelo\n",
        "    def __init__(self, n_inputs):\n",
        "        super(MLP, self).__init__()\n",
        "        # Entrada a la primera capa oculta\n",
        "        self.hidden1 = Linear(n_inputs, 10)\n",
        "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
        "        self.act1 = ReLU()\n",
        "        # Segunda capa oculta\n",
        "        self.hidden2 = Linear(10, 8)\n",
        "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
        "        self.act2 = ReLU()\n",
        "        # Tercera capa oculta y salida\n",
        "        self.hidden3 = Linear(8, 1)\n",
        "        xavier_uniform_(self.hidden3.weight)\n",
        "        self.act3 = Sigmoid()\n",
        "\n",
        "    # Entrada de alimentación hacia delante\n",
        "    def forward(self, X):\n",
        "        # Entrada a la primera capa oculta\n",
        "        X = self.hidden1(X)\n",
        "        X = self.act1(X)\n",
        "        # Segunda capa oculta\n",
        "        X = self.hidden2(X)\n",
        "        X = self.act2(X)\n",
        "        # Tercera capa oculta y salida\n",
        "        X = self.hidden3(X)\n",
        "        X = self.act3(X)\n",
        "        return X"
      ],
      "metadata": {
        "id": "fCFMTN2sUzoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento del modelo**"
      ],
      "metadata": {
        "id": "MgUEy2G0VIZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el modelo\n",
        "def train_model(train_dl, model):\n",
        "    # Definimos la optimización\n",
        "    criterion = BCELoss()\n",
        "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    # Pasamos por las diferentes epocas o repeticiones\n",
        "    for epoch in range(100):\n",
        "        # Enumeramos los mini lotes\n",
        "        for i, (inputs, targets) in enumerate(train_dl):\n",
        "            # Borramos los gradientes\n",
        "            optimizer.zero_grad()\n",
        "            # Calculamos la salida del modelo\n",
        "            yhat = model(inputs)\n",
        "            # Calculamos la pérdida\n",
        "            loss = criterion(yhat, targets)\n",
        "            loss.backward()\n",
        "            # Actualizamos los pesos del modelo\n",
        "            optimizer.step()"
      ],
      "metadata": {
        "id": "Vzro6zazVJ1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluación del modelo**"
      ],
      "metadata": {
        "id": "GtRR1nrTVKq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación del modelo (TENSOR)\n",
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = list(), list()\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        # Evaluamos el modelo con el dataset de prueba\n",
        "        yhat = model(inputs)\n",
        "        # Regresamos un numpy array\n",
        "        yhat = yhat.detach().numpy()\n",
        "        actual = targets.numpy()\n",
        "        actual = actual.reshape((len(actual), 1))\n",
        "        # Redondeamos a valores de clases\n",
        "        yhat = yhat.round()\n",
        "        # Guardamos los valores\n",
        "        predictions.append(yhat)\n",
        "        actuals.append(actual)\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "    # Calculamos la precisión\n",
        "    acc = accuracy_score(actuals, predictions)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "FVIGSlbfVNxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicciones del modelo**"
      ],
      "metadata": {
        "id": "6EgzJoUFVSlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos una predicción de clase para una fila de datos\n",
        "def predict(row, model):\n",
        "    # Convertimos la fila en datos\n",
        "    row = Tensor([row])\n",
        "    # Hacemos la predicción\n",
        "    yhat = model(row)\n",
        "    # Devolvemos un array de numpy\n",
        "    yhat = yhat.detach().numpy()\n",
        "    return yhat"
      ],
      "metadata": {
        "id": "5fSBgDlLVVj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Llamar a los datos**"
      ],
      "metadata": {
        "id": "C0JaUwtZW2GW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
        "train_dl, test_dl = prepare_data(path)\n",
        "print(len(train_dl.dataset), len(test_dl.dataset))\n",
        "# Definimos la red neuronal\n",
        "model = MLP(34)\n",
        "# Entrenamos el modelo\n",
        "train_model(train_dl, model)\n",
        "# Evaluamos el modelo\n",
        "acc = evaluate_model(test_dl, model)\n",
        "print('Precisión: %.3f' % acc)"
      ],
      "metadata": {
        "id": "UQYhOVldW6KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos una predicción\n",
        "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
        "yhat = predict(row, model)\n",
        "print('Predecido: %.3f (class=%d)' % (yhat, yhat.round()))"
      ],
      "metadata": {
        "id": "JyJGzeC1XMYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Guardar el modelo**"
      ],
      "metadata": {
        "id": "k0SbyPSZW5uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.save(model, \"modelopytorch.h5\")\n",
        "# Hacemos una predicción\n",
        "model2 = torch.load(\"modelopytorch.h5\")"
      ],
      "metadata": {
        "id": "WZmRrxgLXP72"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}